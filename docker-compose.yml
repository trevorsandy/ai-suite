volumes:
  n8n_data:
  n8n_worker_data:
  ollama_data:
  qdrant_data:
  opencode_data:
  open_webui_data:
  open_webui_pipelines_data:
  langfuse_postgres_data:
  langfuse_clickhouse_data:
  langfuse_clickhouse_logs:
  langfuse_minio_data:
  redis_valkey_data:
  caddy_data:
  caddy_config_data:

x-n8n: &service-n8n
  image: n8nio/n8n:latest
  environment:
    - NODE_ENV=${NODE_ENV}
    - N8N_DIAGNOSTICS_ENABLED=${N8N_DIAGNOSTICS_ENABLED}
    - N8N_VERSION_NOTIFICATIONS_ENABLED=${N8N_VERSION_NOTIFICATIONS_ENABLED}
    - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}

    - GENERIC_TIMEZONE=${GENERIC_TIMEZONE}
    - TZ=${GENERIC_TIMEZONE}

      # MCP Client node
    - N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE=${N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE}

      # Postgres
    - DB_TYPE=${DB_TYPE}
    - DB_POSTGRESDB_HOST=${POSTGRES_HOST}
    - DB_POSTGRESDB_PORT=${POSTGRES_PORT}
    - DB_POSTGRESDB_DATABASE=${POSTGRES_DB}
    - DB_POSTGRESDB_USER=${POSTGRES_USER}
    - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}

      # Queue / Redis
    - EXECUTIONS_MODE=${EXECUTIONS_MODE}
    - QUEUE_BULL_REDIS_HOST=${QUEUE_BULL_REDIS_HOST}
    - QUEUE_BULL_REDIS_PORT=${QUEUE_BULL_REDIS_PORT}
    - QUEUE_HEALTH_CHECK_ACTIVE=${QUEUE_HEALTH_CHECK_ACTIVE}
    - OFFLOAD_MANUAL_EXECUTIONS_TO_WORKERS=${OFFLOAD_MANUAL_EXECUTIONS_TO_WORKERS}

      # Task runners (2.0)
    - N8N_RUNNERS_ENABLED=${N8N_RUNNERS_ENABLED}
    - N8N_RUNNERS_MODE=${N8N_RUNNERS_MODE}
    - N8N_RUNNERS_BROKER_LISTEN_ADDRESS=${N8N_RUNNERS_BROKER_LISTEN_ADDRESS}
    - N8N_RUNNERS_AUTH_TOKEN=${N8N_RUNNERS_AUTH_TOKEN}
    #- N8N_TASK_BROKER_URL=${N8N_TASK_BROKER_URL}
    #- N8N_COMMAND_RESPONSE_URL=${N8N_COMMAND_RESPONSE_URL}
    #- N8N_TASK_BROKER_PORT=${N8N_TASK_BROKER_PORT}

      # Security dials:
    - N8N_BLOCK_ENV_ACCESS_IN_NODE=${N8N_BLOCK_ENV_ACCESS_IN_NODE}
    - N8N_RESTRICT_FILE_ACCESS_TO=${N8N_RESTRICT_FILE_ACCESS_TO}
    - N8N_GIT_NODE_DISABLE_BARE_REPOS=${N8N_GIT_NODE_DISABLE_BARE_REPOS}
    - N8N_SKIP_AUTH_ON_OAUTH_CALLBACK=${N8N_SKIP_AUTH_ON_OAUTH_CALLBACK}
    - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=${N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS}
      # Remove "ExecuteCommand" and "LocalFileTrigger" nodes from exclude list
    - NODES_EXCLUDE=${NODES_EXCLUDE}

      # HTTPS via reverse proxy
    #- N8N_HOST=${N8N_HOSTNAME:-${N8N_HOST}}
    #- N8N_PORT=${N8N_PORT}
    #- N8N_PROTOCOL=${N8N_PROTOCOL}
    #- N8N_PROXY_HOPS=${N8N_PROXY_HOPS}

    - OLLAMA_HOST=${OLLAMA_HOST}
    - WEBHOOK_URL=${N8N_HOSTNAME:+https://}${N8N_HOSTNAME:-http://localhost:5678}
  env_file:
    - path: .env
      required: true

x-n8n-runner: &service-n8n-runner
  image: n8nio/runners:latest
  environment:
    - N8N_RUNNERS_AUTH_TOKEN=${N8N_RUNNERS_AUTH_TOKEN}
  env_file:
    - path: .env
      required: true

x-ollama: &service-ollama
  image: ollama/ollama:latest
  container_name: ollama
  restart: unless-stopped
  expose:
    - 11434/tcp
  environment:
    - OLLAMA_CONTEXT_LENGTH=8192
    - OLLAMA_FLASH_ATTENTION=1
    - OLLAMA_KV_CACHE_TYPE=q8_0
    - OLLAMA_MAX_LOADED_MODELS=2
  volumes:
    - ollama_data:/root/.ollama

x-init-ollama: &init-ollama
  image: ollama/ollama:latest
  container_name: ollama-pull-llama
  volumes:
    - ollama_data:/root/.ollama
  entrypoint: /bin/sh
  environment:
    - OLLAMA_HOST=ollama:11434
  command:
    - "-c"
    - "sleep 3; ollama pull qwen3:8b; ollama pull llama3.2; ollama pull nomic-embed-text"

name: ai-suite

services:
  open-webui:
    profiles: ["open-webui", "open-webui-all", "n8n", "n8n-all", "ai-all"]
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    expose:
      - 8080/tcp
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL
      - CORS_ALLOW_ORIGIN
      - FORWARDED_ALLOW_IPS
    env_file:
      - path: .env
        required: true

  mcp-gateway:
    profiles: ["open-webui-mcpo", "open-webui", "opencode", "open-webui-all", "n8n", "n8n-all", "ai-all"]
    image: docker/mcp-gateway
    container_name: mcp-gateway
    command:
      - --port=8060
      - --transport=streaming
    expose:
      - 8060/tcp
    volumes:
      - ${DOCKER_SOCKET_LOCATION}.raw:/var/run/docker.sock:ro

  open-webui-mcpo:
    profiles: ["open-webui-mcpo", "open-webui", "open-webui-all", "n8n", "n8n-all", "ai-all"]
    image: ghcr.io/open-webui/mcpo:main
    restart: unless-stopped
    container_name: open-webui-mcpo
    expose:
      - 8090/tcp
    environment:
      - MCPO_HOT_RELOAD
    volumes:
      - ./open-webui/mcpo/config.json:/app/config.json
    command:
      - "--port"
      - "8090"
      - "--config"
      - "/app/config.json"
      - "${MCPO_HOT_RELOAD}"
    depends_on:
      - mcp-gateway
      - open-webui

  open-webui-pipelines:
    profiles: ["open-webui-pipe", "open-webui-all", "ai-all"]
    image: ghcr.io/open-webui/pipelines:main
    restart: unless-stopped
    container_name: open-webui-pipelines
    expose:
      - 9099/tcp
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - open_webui_pipelines_data:/app/pipelines
      - ./open-webui/piplines:/root/.pipelines
    environment:
      - PIPELINES_API_KEY=0p3n-w3bu!
      - PIPELINES_DIR=/root/.pipelines
    depends_on:
      - open-webui

  opencode:
    profiles: ["opencode", "ai-all"]
    container_name: opencode
    image: ghcr.io/sst/opencode
    stdin_open: true
    tty: true
    volumes:
      - opencode_data:/root/.config/opencode
      - ./opencode/opencode.jsonc:/root/.config/opencode/opencode.jsonc
      - ${PROJECTS_PATH:-./opencode}:/root/projects

  flowise:
    profiles: ["flowise", "ai-all"]
    image: flowiseai/flowise
    container_name: flowise
    restart: unless-stopped
    expose:
      - 3001/tcp
    environment:
      - PORT=3001
      - FLOWISE_USERNAME
      - FLOWISE_PASSWORD
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - ./flowise:/root/.flowise
    entrypoint: /bin/sh -c "sleep 3; flowise start"

  n8n-import:
    profiles: ["n8n", "n8n-all", "ai-all"]
    <<: *service-n8n
    container_name: n8n-import
    entrypoint: /bin/sh
    command:
      - "-c"
      - "n8n import:credentials --separate --input=/data/credentials && n8n import:workflow --separate --input=/data/workflows"
    volumes:
      - ./n8n/data:/data

  n8n:
    profiles: ["n8n", "n8n-all", "ai-all"]
    <<: *service-n8n
    container_name: n8n
    restart: unless-stopped
    expose:
      - 5678/tcp
      - 5679/tcp
    volumes:
      - n8n_data:/home/node/.n8n
      - ./n8n/local-files:/home/node/.n8n-files
      - ./n8n/data:/data
      - ${PROJECTS_PATH:-./n8n/local-files}:/home/node/projects
    depends_on: &depends-on-defaults
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      n8n-import:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:5678/health || exit 1 "]
      interval: 5s
      timeout: 5s
      retries: 1
      start_period: 30s

  n8n-runner:
    profiles: ["n8n", "n8n-all", "ai-all"]
    <<: *service-n8n-runner
    container_name: n8n-runner
    restart: unless-stopped
    environment:
      - N8N_RUNNERS_TASK_BROKER_URI=http://n8n:5679
    depends_on:
      - n8n

  n8n-worker:
    profiles: ["n8n", "n8n-all", "ai-all"]
    <<: *service-n8n
    container_name: n8n-worker
    command: worker
    expose:
      - 5678/tcp
    volumes:
      - n8n_worker_data:/home/node/.n8n
    depends_on:
      n8n:
        condition: service_healthy
      <<: *depends-on-defaults

  n8n-worker-runner:
    profiles: ["n8n", "n8n-all", "ai-all"]
    <<: *service-n8n-runner
    container_name: n8n-worker-runner
    restart: unless-stopped
    environment:
      - N8N_RUNNERS_TASK_BROKER_URI=http://n8n-worker:5679
    depends_on:
      - n8n

  postgres:
    profiles: ["n8n", "langfuse", "n8n-all", "ai-all"]
    image: postgres:${POSTGRES_VERSION:-17}
    container_name: postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 3s
      timeout: 3s
      retries: 10
    expose:
      - 5432/tcp
    environment:
      POSTGRES_DB: postgres
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - langfuse_postgres_data:/var/lib/postgresql/data

  qdrant:
    profiles: ["n8n", "n8n-all", "ai-all"]
    image: qdrant/qdrant
    container_name: qdrant
    restart: unless-stopped
    expose:
      - 6333/tcp
      - 6334/tcp
    volumes:
      - qdrant_data:/qdrant/storage

  neo4j:
    profiles: ["neo4j", "ai-all"]
    image: neo4j:latest
    container_name: neo4j
    volumes:
      - ./neo4j/logs:/logs
      - ./neo4j/config:/config
      - ./neo4j/data:/data
      - ./neo4j/plugins:/plugins
    expose:
      - 7473/tcp
      - 7474/tcp
      - 7687/tcp
    environment:
      - NEO4J_AUTH=${NEO4J_AUTH:-neo4j/password}
    restart: always

  caddy:
    profiles: ["caddy", "ai-all"]
    image: docker.io/library/caddy:2-alpine
    container_name: caddy
    restart: unless-stopped
    ports:
      - 80:80/tcp
      - 443:443/tcp
    expose:
      - 2019/tcp
      - 443/tcp
      - 443/udp
      - 80/tcp
    volumes:
      - ./caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./caddy/addons:/etc/caddy/addons:ro
      - caddy_data:/data:rw
      - caddy_config_data:/config:rw
    environment:
      - N8N_HOSTNAME=${N8N_HOSTNAME:-":8001"}
      - WEBUI_HOSTNAME=${WEBUI_HOSTNAME:-":8002"}
      - FLOWISE_HOSTNAME=${FLOWISE_HOSTNAME:-":8003"}
      - OLLAMA_HOSTNAME=${OLLAMA_HOSTNAME:-":8004"}
      - SUPABASE_HOSTNAME=${SUPABASE_HOSTNAME:-":8005"}
      - SEARXNG_HOSTNAME=${SEARXNG_HOSTNAME:-":8006"}
      - LANGFUSE_HOSTNAME=${LANGFUSE_HOSTNAME:-":8007"}
      - NEO4J_HOSTNAME=${NEO4J_HOSTNAME:-":8008"}
      - LETSENCRYPT_EMAIL=${LETSENCRYPT_EMAIL:-internal}
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  langfuse-worker:
    profiles: ["langfuse", "ai-all"]
    image: langfuse/langfuse-worker:3
    container_name: langfuse-worker
    restart: always
    depends_on: &langfuse-depends-on
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    expose:
     - 3030/tcp
    environment: &langfuse-worker-env
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/postgres
      SALT: ${LANGFUSE_SALT}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-true}
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: ${LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES:-true}
      CLICKHOUSE_MIGRATION_URL: ${CLICKHOUSE_MIGRATION_URL:-clickhouse://clickhouse:9000}
      CLICKHOUSE_URL: ${CLICKHOUSE_URL:-http://clickhouse:8123}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      CLICKHOUSE_CLUSTER_ENABLED: ${CLICKHOUSE_CLUSTER_ENABLED:-false}
      LANGFUSE_S3_EVENT_UPLOAD_BUCKET: ${LANGFUSE_S3_EVENT_UPLOAD_BUCKET:-langfuse}
      LANGFUSE_S3_EVENT_UPLOAD_REGION: ${LANGFUSE_S3_EVENT_UPLOAD_REGION:-auto}
      LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID: ${LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT: ${LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT:-http://minio:9000}
      LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE: ${LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE:-true}
      LANGFUSE_S3_EVENT_UPLOAD_PREFIX: ${LANGFUSE_S3_EVENT_UPLOAD_PREFIX:-events/}
      LANGFUSE_S3_MEDIA_UPLOAD_BUCKET: ${LANGFUSE_S3_MEDIA_UPLOAD_BUCKET:-langfuse}
      LANGFUSE_S3_MEDIA_UPLOAD_REGION: ${LANGFUSE_S3_MEDIA_UPLOAD_REGION:-auto}
      LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID: ${LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT: ${LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT:-http://localhost:9090}
      LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE: ${LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE:-true}
      LANGFUSE_S3_MEDIA_UPLOAD_PREFIX: ${LANGFUSE_S3_MEDIA_UPLOAD_PREFIX:-media/}
      LANGFUSE_S3_BATCH_EXPORT_ENABLED: ${LANGFUSE_S3_BATCH_EXPORT_ENABLED:-false}
      LANGFUSE_S3_BATCH_EXPORT_BUCKET: ${LANGFUSE_S3_BATCH_EXPORT_BUCKET:-langfuse}
      LANGFUSE_S3_BATCH_EXPORT_PREFIX: ${LANGFUSE_S3_BATCH_EXPORT_PREFIX:-exports/}
      LANGFUSE_S3_BATCH_EXPORT_REGION: ${LANGFUSE_S3_BATCH_EXPORT_REGION:-auto}
      LANGFUSE_S3_BATCH_EXPORT_ENDPOINT: ${LANGFUSE_S3_BATCH_EXPORT_ENDPOINT:-http://minio:9000}
      LANGFUSE_S3_BATCH_EXPORT_EXTERNAL_ENDPOINT: ${LANGFUSE_S3_BATCH_EXPORT_EXTERNAL_ENDPOINT:-http://localhost:9090}
      LANGFUSE_S3_BATCH_EXPORT_ACCESS_KEY_ID: ${LANGFUSE_S3_BATCH_EXPORT_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_BATCH_EXPORT_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      LANGFUSE_S3_BATCH_EXPORT_FORCE_PATH_STYLE: ${LANGFUSE_S3_BATCH_EXPORT_FORCE_PATH_STYLE:-true}
      LANGFUSE_INGESTION_QUEUE_DELAY_MS: ${LANGFUSE_INGESTION_QUEUE_DELAY_MS:-}
      LANGFUSE_INGESTION_CLICKHOUSE_WRITE_INTERVAL_MS: ${LANGFUSE_INGESTION_CLICKHOUSE_WRITE_INTERVAL_MS:-}
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
      REDIS_AUTH: ${REDIS_AUTH:-LOCALONLYREDIS}
      REDIS_TLS_ENABLED: ${REDIS_TLS_ENABLED:-false}
      REDIS_TLS_CA: ${REDIS_TLS_CA:-/certs/ca.crt}
      REDIS_TLS_CERT: ${REDIS_TLS_CERT:-/certs/redis.crt}
      REDIS_TLS_KEY: ${REDIS_TLS_KEY:-/certs/redis.key}

  langfuse-web:
    profiles: ["langfuse", "ai-all"]
    image: langfuse/langfuse:3
    container_name: langfuse-web
    restart: always
    depends_on: *langfuse-depends-on
    expose:
      - 3000/tcp
    environment:
      <<: *langfuse-worker-env
      NEXTAUTH_URL: http://localhost:3002
      NEXTAUTH_SECRET: ${NEXTAUTH_SECRET}
      LANGFUSE_INIT_ORG_ID: ${LANGFUSE_INIT_ORG_ID:-}
      LANGFUSE_INIT_ORG_NAME: ${LANGFUSE_INIT_ORG_NAME:-}
      LANGFUSE_INIT_PROJECT_ID: ${LANGFUSE_INIT_PROJECT_ID:-}
      LANGFUSE_INIT_PROJECT_NAME: ${LANGFUSE_INIT_PROJECT_NAME:-}
      LANGFUSE_INIT_PROJECT_PUBLIC_KEY: ${LANGFUSE_INIT_PROJECT_PUBLIC_KEY:-}
      LANGFUSE_INIT_PROJECT_SECRET_KEY: ${LANGFUSE_INIT_PROJECT_SECRET_KEY:-}
      LANGFUSE_INIT_USER_EMAIL: ${LANGFUSE_INIT_USER_EMAIL:-}
      LANGFUSE_INIT_USER_NAME: ${LANGFUSE_INIT_USER_NAME:-}
      LANGFUSE_INIT_USER_PASSWORD: ${LANGFUSE_INIT_USER_PASSWORD:-}

  clickhouse:
    profiles: ["langfuse", "ai-all"]
    image: clickhouse/clickhouse-server
    container_name: clickhouse
    restart: always
    user: "101:101"
    expose:
      - 8123/tcp
      - 9000/tcp
      - 9009/tcp
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: clickhouse
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
    volumes:
      - langfuse_clickhouse_data:/var/lib/clickhouse
      - langfuse_clickhouse_logs:/var/log/clickhouse-server
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 1s

  minio:
    profiles: ["langfuse", "ai-all"]
    image: minio/minio
    container_name: minio
    restart: always
    entrypoint: sh
    # create the 'langfuse' bucket before starting the service
    command: -c 'mkdir -p /data/langfuse && minio server --address ":9000" --console-address ":9001" /data'
    expose:
      - 9000/tcp
      - 9001/tcp
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - langfuse_minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 1s
      timeout: 5s
      retries: 5
      start_period: 1s

  redis:
    profiles: ["n8n", "langfuse", "ai-all"]
    image: docker.io/valkey/valkey:8-alpine
    container_name: redis
    command: valkey-server --save 30 1 --loglevel warning
    restart: unless-stopped
    expose:
      - 6379/tcp
    volumes:
      - redis_valkey_data:/data
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 3s
      timeout: 10s
      retries: 10

  searxng:
    profiles: ["searxng", "ai-all"]
    image: docker.io/searxng/searxng:latest
    container_name: searxng
    restart: unless-stopped
    expose:
      - 8080/tcp
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  ollama-cpu:
    profiles: ["cpu"]
    <<: *service-ollama

  ollama-gpu:
    profiles: ["gpu-nvidia"]
    <<: *service-ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama-gpu-amd:
    profiles: ["gpu-amd"]
    <<: *service-ollama
    image: ollama/ollama:rocm
    devices:
      - "/dev/kfd"
      - "/dev/dri"

  ollama-pull-llama-cpu:
    profiles: ["cpu"]
    <<: *init-ollama
    depends_on:
      - ollama-cpu

  ollama-pull-llama-gpu:
    profiles: ["gpu-nvidia"]
    <<: *init-ollama
    depends_on:
      - ollama-gpu

  ollama-pull-llama-gpu-amd:
    profiles: ["gpu-amd"]
    <<: *init-ollama
    image: ollama/ollama:rocm
    depends_on:
      - ollama-gpu-amd
